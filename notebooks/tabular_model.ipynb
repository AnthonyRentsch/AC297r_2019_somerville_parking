{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras\n",
    "\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, GaussianNoise\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoother(aerial, gsv):\n",
    "    mapping = {\n",
    "        (0, 0): 0,\n",
    "        (1, 0): 2,\n",
    "        (2, 0): 0,\n",
    "        (0, 1): 2,\n",
    "        (1, 1): 1,\n",
    "        (2, 1): 1,\n",
    "        (0, 2): 0,\n",
    "        (1, 2): 1,\n",
    "        (2, 2): 2\n",
    "    }\n",
    "    return mapping[(int(aerial), int(gsv))]\n",
    "\n",
    "def clean_labels_redfin(labels):\n",
    "    labels = labels[['has_parking','MBL']]\n",
    "    return labels\n",
    "\n",
    "def clean_labels_hand(labels):\n",
    "    # smooth labels\n",
    "    # labels = labels[~((labels['AERIAL_Driveway'] == 2) & (labels['GSV_Driveway'] == 2))]\n",
    "    labels['final_label']= labels.apply(lambda x: label_smoother(x['AERIAL_Driveway'],x['GSV_Driveway']), axis = 1)\n",
    "    labels = labels[['final_label', 'MBL']]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_redfin_path = '../data/redfin_clean.csv'\n",
    "labels_hand_path = '../data/training/all_labels.csv'\n",
    "#labels_hand_path_2 = '../data/additional_training_labels_120319.csv'\n",
    "garage_path = '../data/garage.csv'\n",
    "tabular_path = '../data/residence_addresses_googlestreetview_clean.csv'\n",
    "\n",
    "# import dataframe with filenames and labels\n",
    "labels_redfin = pd.read_csv(labels_redfin_path)\n",
    "labels_redfin = labels_redfin.fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import data by hand\n",
    "labels_hand = pd.read_csv(labels_hand_path, index_col = 0)[['MBL','AERIAL_Driveway', 'GSV_Driveway']]\n",
    "#labels_hand_2 = pd.read_csv(labels_hand_path_2)[['MBL','AERIAL_Driveway', 'GSV_Driveway']]\n",
    "#labels_hand = pd.concat([labels_hand, labels_hand_2], axis = 0)\n",
    "labels_hand = labels_hand.fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hand.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load garages\n",
    "garages = pd.read_csv(garage_path, index_col=0)\n",
    "\n",
    "# load tabular data\n",
    "tabular = pd.read_csv(tabular_path, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_redfin_clean = clean_labels_redfin(labels_redfin)\n",
    "labels_hand_clean = clean_labels_hand(labels_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hand_clean.final_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = labels_hand.final_label.value_counts()\n",
    "percent_driveway = label_count[1]/(label_count[1] + label_count[0]) \n",
    "percent_driveway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_clean = labels_hand_clean.merge(labels_redfin_clean, how = 'outer').merge(garages, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_clean.has_parking = labels_clean.apply(\n",
    "    lambda row: row.has_parking if not np.isnan(row.has_parking) else \n",
    "                row.HAS_GARAGE if not np.isnan(row.HAS_GARAGE) else row.final_label , axis = 1\n",
    ")\n",
    "labels_clean = labels_clean.drop(['final_label', 'HAS_GARAGE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_clean.to_csv('../data/labels_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(df):\n",
    "    # upsample\n",
    "    n1 = sum((df.has_parking == 1) | (df.has_parking == 0.9))\n",
    "    n0 = sum((df.has_parking == 0) | (df.has_parking == 0.1))\n",
    "    labels0 = df[(df.has_parking == 0) | (df.has_parking == 0.1)]\n",
    "    labels0_upsample = labels0.sample(n1 - n0, replace = True)\n",
    "    labels_all_upsampled = pd.concat([df, labels0_upsample])\n",
    "    return labels_all_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labels_clean.merge(tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/df_training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#scaler = StandardScaler()\n",
    "#df_viz = scaler.fit_transform(df.drop('MBL', axis = 1))\n",
    "df_viz = pd.DataFrame(df, columns = df.columns.drop(['MBL']))\n",
    "df_viz = upsample(df_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clm in df_viz.columns:\n",
    "    try:\n",
    "        sns.distplot(df_viz[df.has_parking == 0][clm], label = 'no driveway', rug=True, hist=False)\n",
    "        sns.distplot(df_viz[df.has_parking == 1][clm], label = 'driveway', rug=True, hist=False)\n",
    "        plt.legend()\n",
    "        plt.title(clm)\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, x in enumerate(df_viz.corr()['has_parking']):\n",
    "    print(df_viz.corr()['has_parking'].index[idx])\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.has_parking != 2]\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size = 0.2)\n",
    "\n",
    "df_train_up = upsample(df_train)\n",
    "df_test_up = upsample(df_test)\n",
    "\n",
    "\n",
    "X_train, y_train = df_train_up[tabular.columns].drop('MBL', axis = 1), df_train_up['has_parking']\n",
    "X_test, y_test = df_test_up[tabular.columns].drop('MBL', axis = 1), df_test_up['has_parking']\n",
    "\n",
    "X_train_original, y_train_original = df_train[tabular.columns].drop('MBL', axis = 1), df_train['has_parking']\n",
    "X_test_original, y_test_original = df_test[tabular.columns].drop('MBL', axis = 1), df_test['has_parking']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train_original = scaler.transform(X_train_original)\n",
    "X_test_original = scaler.transform(X_test_original)\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "with open(f'../models/scaler_{now}.txt', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# poly = PolynomialFeatures(degree = 1)\n",
    "# X_train = poly.fit_transform(X_train)\n",
    "# X_test = poly.transform(X_test)\n",
    "\n",
    "y_test = y_test.apply(lambda x: 1 if x > .5 else 0)\n",
    "y_train = y_train.apply(lambda x: 1 if x > .5 else 0)\n",
    "y_train_original = y_train_original.apply(lambda x: 1 if x > .5 else 0)\n",
    "y_test_original = y_test_original.apply(lambda x: 1 if x > .5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = 1e-3, penalty = 'l2', max_iter = 300)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('validation stats on upsampled test set:')\n",
    "# validate on upsampled\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# validate on original data\n",
    "print('validation stats on regular test set:')\n",
    "y_pred_original = lr.predict(X_test_original)\n",
    "print(confusion_matrix(y_test_original,y_pred_original))\n",
    "print(classification_report(y_test_original,y_pred_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "with open(f'../models/logreg_{now}.txt', 'wb') as f:\n",
    "    pickle.dump(lr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate on original certain\n",
    "# validate on original data\n",
    "y_pred_original_proba = lr.predict_proba(X_test_original)\n",
    "y_pred_original = lr.predict(X_test_original)\n",
    "threshold = .2\n",
    "is_certain_pred = np.absolute(y_pred_original_proba[:,1] - 0.5) > threshold\n",
    "print(classification_report(y_test_original[is_certain_pred],y_pred_original[is_certain_pred]))\n",
    "print(sum(is_certain_pred)/len(is_certain_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(y_pred_original_proba[:,1][y_test_original == 1])\n",
    "sns.distplot(y_pred_original_proba[:,1][y_test_original == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_dict = dict(zip(df_test[tabular.columns].drop('MBL', axis = 1).columns, lr.coef_.reshape(-1)))\n",
    "\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/ Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features for interaction with lasso\n",
    "lr = LogisticRegression(C = 0.006, penalty = 'l1', max_iter = 300, solver = 'liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "# validate on upsampled\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# validate on original data\n",
    "print('validation stats on regular test set:')\n",
    "y_pred_original = lr.predict(X_test_original)\n",
    "print(confusion_matrix(y_test_original,y_pred_original))\n",
    "print(classification_report(y_test_original,y_pred_original))\n",
    "\n",
    "interaction_columns = [clm \n",
    "                       for idx, clm in enumerate(df_test[tabular.columns].drop('MBL', axis = 1)) \n",
    "                       if lr.coef_.reshape(-1)[idx] > 0]\n",
    "\n",
    "training_clms = df_train_up[tabular.columns].drop('MBL', axis = 1).columns\n",
    "interaction_clms_idx = [idx for idx, clm in enumerate(training_clms) if clm in interaction_columns]\n",
    "non_interaction_clms_idx = [idx for idx, clm in enumerate(training_clms) if clm not in interaction_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = 2)\n",
    "X_train_interact = poly.fit_transform(X_train[:,interaction_clms_idx])\n",
    "X_test_interact = poly.transform(X_test[:,interaction_clms_idx])\n",
    "X_test_original_interact = poly.transform(X_test_original[:,interaction_clms_idx])\n",
    "\n",
    "\n",
    "X_train_interact = np.concatenate([X_train_interact, X_train[:,non_interaction_clms_idx]], axis = 1)\n",
    "X_test_interact = np.concatenate([X_test_interact, X_test[:,non_interaction_clms_idx]], axis = 1)\n",
    "X_test_original_interact = np.concatenate([X_test_original_interact, X_test_original[:,non_interaction_clms_idx]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_interact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = 0.003, penalty = 'l2', max_iter = 300)\n",
    "lr.fit(X_train_interact, y_train)\n",
    "\n",
    "# validate on upsampled\n",
    "y_pred = lr.predict(X_test_interact)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# validate on original data\n",
    "y_pred_original = lr.predict(X_test_original_interact)\n",
    "print(confusion_matrix(y_test_original,y_pred_original))\n",
    "print(classification_report(y_test_original,y_pred_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test == y_pred)/len(y_test==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "y_pred_original = svclassifier.predict(X_test_original)\n",
    "print(confusion_matrix(y_test_original, y_pred_original))\n",
    "print(classification_report(y_test_original, y_pred_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "with open(f'../models/svc_{now}.txt', 'wb') as f:\n",
    "    pickle.dump(svclassifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = df.drop(['MBL','AERIAL_Driveway', 'GSV_Driveway'], axis = 1)\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "df.has_parking = df.has_parking.apply(lambda x: 1 if x > .5 else 0)\n",
    "\n",
    "train = df\n",
    "target = 'has_parking'\n",
    "IDcol = 'MBL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = train.has_parking.value_counts()\n",
    "scale = count[0]/count[1]\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, cv_folds=5, early_stopping_rounds=300):\n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "        metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    print(cvresult.shape[0])\n",
    "    return cvresult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=scale,\n",
    " seed=27)\n",
    "\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "best_n_estimators = modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[5,6,7]\n",
    "}\n",
    "\n",
    "gsearch1 = (\n",
    "    GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=best_n_estimators, max_depth=5,\n",
    "                 min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                 objective= 'binary:logistic', nthread=4, scale_pos_weight=scale, seed=27), \n",
    "    param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    ")\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch2 = GridSearchCV(\n",
    "    estimator = XGBClassifier( \n",
    "        learning_rate =0.1, \n",
    "        n_estimators=best_n_estimators, \n",
    "        max_depth=gsearch1.best_params_['max_depth'],\n",
    "        min_child_weight=gsearch1.best_params_['min_child_weight'], \n",
    "        gamma=0, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic', \n",
    "        nthread=4, \n",
    "        scale_pos_weight=scale,\n",
    "        seed=27), \n",
    "    param_grid = param_test2, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    iid=False,\n",
    "    cv=5\n",
    ")\n",
    "gsearch2.fit(train[predictors],train[target])\n",
    "gsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBClassifier(\n",
    "    learning_rate =0.1, \n",
    "    n_estimators=1000, \n",
    "    max_depth=gsearch1.best_params_['max_depth'],\n",
    "    min_child_weight=gsearch1.best_params_['min_child_weight'], \n",
    "    gamma=gsearch2.best_params_['gamma'], \n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic', \n",
    "    nthread=4, \n",
    "    scale_pos_weight=scale,\n",
    "    seed=27\n",
    ")\n",
    "best_n_estimators = modelfit(xgb2, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3 = XGBClassifier(\n",
    "    learning_rate =0.1, \n",
    "    n_estimators=best_n_estimators, \n",
    "    max_depth=gsearch1.best_params_['max_depth'],\n",
    "    min_child_weight=gsearch1.best_params_['min_child_weight'], \n",
    "    gamma=gsearch2.best_params_['gamma'], \n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic', \n",
    "    nthread=4, \n",
    "    scale_pos_weight=scale,\n",
    "    seed=27\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "train, test = train_test_split(df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3.fit(train[predictors], train[target])\n",
    "y_pred = xgb3.predict(test[predictors])\n",
    "sum(y_pred == test[target])/len(test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb3.predict(test[predictors])\n",
    "print(classification_report(test[target],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 40\n",
    "layers = 6\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GaussianNoise(0.1, input_shape = (X_train.shape[1], )))\n",
    "for _ in range(layers):\n",
    "    model.add(Dense(50, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "adam = Adam(lr = 1e-4)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'bootstrap' : [True, False],\n",
    "    'n_estimators' : [16, 64, 256, 1024],\n",
    "    'max_depth' : [3,4,5,6]\n",
    "}\n",
    "\n",
    "rf_up = GridSearchCV(RandomForestClassifier(), params, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_up.fit(X_train, y_train)\n",
    "y_pred = rf_up.predict(X_test)\n",
    "\n",
    "print('validation stats on upsampled test set:')\n",
    "# validate on upsampled\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# validate on original data\n",
    "print('validation stats on regular test set:')\n",
    "y_pred_original = rf_up.predict(X_test_original)\n",
    "print(confusion_matrix(y_test_original,y_pred_original))\n",
    "print(classification_report(y_test_original,y_pred_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test==y_pred)/len(y_test==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_up.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_up.predict(X_test)\n",
    "y_pred_proba = rf_up.predict_proba(X_test)\n",
    "certain = y_pred_proba[:,1] > 0.6\n",
    "\n",
    "print('validation stats on upsampled test set:')\n",
    "# validate on upsampled\n",
    "print(confusion_matrix(y_test[certain], y_pred[certain]))\n",
    "print(classification_report(y_test[certain], y_pred[certain]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../models/random_forest_{now}.txt', 'wb') as f:\n",
    "    pickle.dump(rf_up, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_up.predict(X_test[0:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "res = calibration_curve(y_test, y_pred_proba[:,1], n_bins=10)\n",
    "print(res)\n",
    "plt.plot(res[1], res[0])\n",
    "plt.plot(np.linspace(0,1,20), np.linspace(0,1,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "models = [('rf', rf_up),('svc', svclassifier), ('lr',lr)]\n",
    "\n",
    "stack = StackingClassifier(models)\n",
    "stack.fit(X_train, y_train)\n",
    "y_pred = stack.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('validation stats on upsampled test set:')\n",
    "# validate on upsampled\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# validate on original data\n",
    "print('validation stats on regular test set:')\n",
    "y_pred_original = stack.predict(X_test_original)\n",
    "print(confusion_matrix(y_test_original,y_pred_original))\n",
    "print(classification_report(y_test_original,y_pred_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_no_mbl = tabular.drop('MBL', axis =1)\n",
    "tabular_no_na = tabular_no_mbl.fillna(tabular_no_mbl.mean())\n",
    "y_prob = rf_up.predict_proba(scaler.transform(tabular_no_na))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all = pd.DataFrame()\n",
    "predictions_all['MBL'] = tabular['MBL']\n",
    "predictions_all['no_driveway'] = y_prob[:,0]\n",
    "predictions_all['yes_driveway'] = y_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all.to_csv('../data/predictions_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['MBL','has_parking'], axis = 1).columns\n",
    "\n",
    "importances = rf_up.best_estimator_.feature_importances_\n",
    "\n",
    "feature_imp = dict(zip(features, importances))\n",
    "\n",
    "import operator\n",
    "sorted(feature_imp.items(), key=operator.itemgetter(1), reverse = True)[:20]\n",
    "pd.DataFrame(sorted(feature_imp.items(), key=operator.itemgetter(1), reverse = True)[:20]).to_csv('../data/feature_imp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
